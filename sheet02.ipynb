{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 02: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "By now everyone should have found a group. If someone still has none but wants to participate in the course please contact one of the tutors.\n",
    "\n",
    "This weeks sheet should be solved and handed in before the end of **Sunday, April 24, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whom ever of us you run into first. Please upload your results to your group's studip folder.\n",
    "\n",
    "If, during the programming tasks, you run into a `NameError`, make sure that you executed all prior code cells beforehand. Later cells might rely on variables and function from prior cells. To see all currently defined variables you can make use of the `%whos` [magic function](https://github.com/lmmx/devnotes/wiki/IPython-'magic'-function-documentation#whos) anywhere in code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Decision Trees [2 Points]\n",
    "Draw the decision trees for the following boolean functions. Either use pen and paper and bring the results to the feedback session or employ your ASCII artist within below. \n",
    "\n",
    "Note: $\\oplus := xor$, that means one of the operands has to be true, while the other one has to be false:\n",
    "\n",
    "$\\oplus$ | $B$ | $\\neg B$\n",
    "---------|-----|---------\n",
    "$A$      |  f  |    t\n",
    "$\\neg A$ |  t  |    f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) $\\neg A \\wedge B$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) $A \\oplus B$ "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) $A \\vee (B \\wedge C) \\vee (\\neg C \\wedge D)$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) $(A \\rightarrow (B \\wedge \\neg C)) \\vee (A \\wedge B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Entropy and Information Gain [8 Points]\n",
    "\n",
    "In many machine learning applications it is crucial to determine which criterions are necessary for a good classification. Decision trees have those criterions close to the root, imposing an order from significant to less significant criterions. One way to select the most important criterion is to compare its information gain or its entropy to others. The following dataset is a hands-on example for this method. \n",
    "\n",
    "Consider the following attributes with their possible values:\n",
    "\n",
    "  * $raining = \\{yes, no\\}$\n",
    "  * $tired = \\{yes, no\\}$\n",
    "  * $late = \\{yes, no\\}$\n",
    "  * $distance = \\{short, medium, long\\}$\n",
    "\n",
    "And a training data set consisting of those attributes:\n",
    "\n",
    "| #  | raining | tired | late | distance | attend_party |\n",
    "|----|---------|-------|------|----------|--------------|\n",
    "| 1  | yes     | no    | no   | short    | **yes**      |\n",
    "| 2  | yes     | no    | yes  | medium   | **no**       |\n",
    "| 3  | no      | yes   | no   | long     | **no**       |\n",
    "| 4  | yes     | yes   | yes  | short    | **no**       |\n",
    "| 5  | yes     | no    | no   | short    | **yes**      |\n",
    "| 6  | no      | no    | no   | medium   | **yes**      |\n",
    "| 7  | no      | yes   | no   | long     | **no**       |\n",
    "| 8  | yes     | no    | yes  | short    | **no**       |\n",
    "| 9  | yes     | yes   | no   | short    | **yes**      |\n",
    "| 10 | no      | yes   | no   | medium   | **no**       |\n",
    "| 11 | no      | yes   | no   | long     | **no**       |\n",
    "| 12 | no      | yes   | yes  | short    | **no**       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Build the root node of a decision tree from the training samples given in the table above by calculating the information gain for all four attributes (raining, tired, late, distance).\n",
    "\n",
    "$$Gain(S,A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|}Entropy(S_v)$$\n",
    "\n",
    "$$Entropy(S) = -p_{\\oplus} log_{2} p_{\\oplus} - p_{\\ominus} log_{2} p_{\\ominus}$$\n",
    "\n",
    "$S$ is the set of all data samples. $S_v$ is the subset for which attribute $A$ has value $v$. An example for attribute **tired** with value $yes$ would be:\n",
    "$$|S_{yes}| = 7, S_{yes}:[1+, 6−]$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gain\n",
    "raining   Gain = 0.093284623230312502 S_yes   = [6+,6-]\n",
    "tired     Gain = 0.16859063219201986  S_yes   = [7+,5-]\n",
    "late      Gain = 0.25162916738782293  S_yes   = [4+,8-]\n",
    "distance  Gain = 0.1887218755408675   S_short = [6-,3#,3+]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Perform the same calculation as in **a)** but use the gain ratio instead of the information gain. Does the result for the root node change?\n",
    "\n",
    "$$GainRatio(S,A) = \\frac{Gain(S,A)}{SplitInformation(S,A)}$$\n",
    "\n",
    "$$SplitInformation(S,A) = - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} log_{2} \\frac{S_{v}}{S}$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "raining: \n",
    "Gain =             0.093\n",
    "SplitInformation = -6/12 log2 6/12 - 6/12 log2 6/12 = 1.0\n",
    "\n",
    "GainRatio = Gain = 0.093\n",
    "\n",
    "tired:\n",
    "Gain = 0.169\n",
    "SplitInformation = -7/12 log2 7/12 - 5/12 log2 5/12 = 0.98\n",
    "\n",
    "GainRatio = 0.172\n",
    "\n",
    "late:\n",
    "Gain = 0.252\n",
    "SplitInformation = -4/12 log2 4/12 - 8/12 log2 8/12 = 0.918\n",
    "\n",
    "GainRatio = 0.275\n",
    "\n",
    "distance:\n",
    "Gain = 0.189\n",
    "SplitInformation = -6/12 log2 6/12 -3/12 log2 3/12 -3/12 log2 3/12 = 1.5\n",
    "\n",
    "GainRatio = 0.126\n",
    "\n",
    "The root node stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: ID3 algorithm [5 Points]\n",
    "\n",
    "Implement the following two functions in Python. Take a look at the `assert`s to see how the function should behave. An assert is a condition that your function is required to pass. Most of the conditions here are taken from the lecture slides (ML-03, Slide 12 & 13). Don't worry if you do not get all asserts to pass, just comment the failing ones out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Entropy\n",
    "\n",
    "$$Entropy(S) = - \\sum_{i=1...c} p_i log_2 p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "def entropy(S):\n",
    "    \"\"\"\n",
    "    Calculate the entropy for a given target value set. \n",
    "    S: List of target classes for specific observations.\n",
    "    \"\"\"\n",
    "    prop = np.divide(np.array([S.count(i) for i in np.unique(S)],dtype='float'),len(S))\n",
    "    return -sum([i*log(i,2) for i in prop])\n",
    "\n",
    "# See ML-03, Slide 12 & 13\n",
    "assert entropy([1,1,1,0,0,0]) == 1.0\n",
    "assert round(entropy([1,1,1,1,0,0,0]), 3) == 0.985\n",
    "assert round(entropy([1,1,1,1,1,1,0]), 3) == 0.592\n",
    "assert round(entropy([1,1,1,1,1,1,0,0]), 3) == 0.811\n",
    "assert round(entropy([2,2,1,1,0,0]), 3) == 1.585\n",
    "assert round(entropy([2,2,2,1,0]), 3) == 1.371\n",
    "assert round(entropy([2,2,2,0,0]), 3) == 0.971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)  Information Gain\n",
    "\n",
    "$$Gain(S,A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def gain(S, A):\n",
    "    \"\"\"\n",
    "    Calculates the expected reduction in entropy due to sorting on A.\n",
    "    S: Target classes for observations in A.\n",
    "    A: Observations.\n",
    "    \"\"\"\n",
    "    Sum = 0\n",
    "    for v in np.unique(A):\n",
    "        Sv   = [x for i,x in enumerate(S) if A[i]==v]\n",
    "        Sum += len(Sv)/len(S) * entropy(Sv)\n",
    "\n",
    "    return entropy(S) - Sum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# See ML-03, Slide 12 & 13\n",
    "assert_S_ = [0,0,1,1,1,0,1,0,1,1,1,1,1,0]\n",
    "assert round(gain(assert_S_, [1,1,1,1,0,0,0,1,0,0,0,1,0,1]), 3) == .152\n",
    "assert round(gain(assert_S_, [0,1,0,0,0,1,1,0,0,0,1,1,0,1]), 3) == .048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) ID3\n",
    "\n",
    "In the next two cells we have implemented the ID3 algorithm following the pseudocode from [Wikipedia](https://en.wikipedia.org/wiki/ID3_algorithm#Pseudocode) -- it relies on your two functions from above, `entropy` and `gain`. You do not have to understand the code completly! We recommend that you read through it, making your way from comment to comment, but understanding every line is not necessary for this exercise.\n",
    "\n",
    "Below the algorithm's cell it is applied to two data sets. Run those and discuss the differences. For which data set is the ID3 algorithm better suited and why? (Enter your answer in the cell below the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class Node(namedtuple('Node', 'label children')):\n",
    "    \"\"\"\n",
    "    A small node representation with a pretty string representation.\n",
    "    \"\"\"\n",
    "    def __str__(self, level=0):\n",
    "        return_str ='{}{!s}\\n'.format(' ' * level * 4, self.label)\n",
    "        for child in self.children:\n",
    "            return_str += child.__str__(level + 1)\n",
    "        return return_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def id3(examples, attributes, target_attribute = None): \n",
    "    \"\"\"\n",
    "    Calculate a tree of Nodes (fields: label [string], children [list]) \n",
    "    using the ID3 algorithm found as pseudocode on Wikipedia. Including comments (currently).\n",
    "    \"\"\"\n",
    "    # Create a root node for the tree\n",
    "    # If all examples are positive, Return the single-node tree Root, with label = \n",
    "    # If all examples are negative, Return the single-node tree Root, with label = -.\n",
    "    if all(target == examples['targets'][0] for target in examples['targets']):\n",
    "        return Node('Result: {!s}'.format(examples['target_names'][examples['targets'][0]]), [])\n",
    "    \n",
    "    # If number of predicting attributes is empty, then Return the single node tree Root,\n",
    "    # with label = most common value of the target attribute in the examples.\n",
    "    if len(attributes) == 0:\n",
    "        attr = Counter(data_sample[target_attribute] for data_sample in examples['data']).most_common(1)\n",
    "        return Node('Attribute: {!s}, {!s} occurences'.format(examples['attributes'][target_attribute], attr), [])\n",
    "    \n",
    "    # A <-- The Attribute that best classifies examples.\n",
    "    gains = [gain(examples['targets'], [r[attribute] for r in examples['data']]) for attribute in attributes]\n",
    "    attribute = attributes[gains.index(max(gains))]\n",
    "\n",
    "    # Create a root node for the tree (2)\n",
    "    # Decision Tree attribute for Root = A.\n",
    "    root = Node('Attribute: {!s} (gain {!s})'.format(examples['attributes'][attribute], round(min(gains), 4)), [])\n",
    "    \n",
    "    # Otherwise Begin\n",
    "    # For each possible value, vi, of A,\n",
    "    for vi in set(data_sample[attribute] for data_sample in examples['data']):\n",
    "        # Add a new tree branch below Root, corresponding to the test A = vi.\n",
    "        child = Node('Value: {!s}'.format(vi), [])\n",
    "        root.children.append(child)\n",
    "        \n",
    "        # Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "        vi_indices = [idx for idx, data_sample in enumerate(examples['data']) if data_sample[attribute] == vi]\n",
    "        examples_vi = dict(examples)\n",
    "        examples_vi['data'] = [examples['data'][i] for i in vi_indices]\n",
    "        examples_vi['targets'] = [examples['targets'][i] for i in vi_indices]\n",
    "        \n",
    "        # If Examples(vi) is empty\n",
    "        if len(examples_vi['data']) == 0:\n",
    "            # Then below this new branch add a leaf node with label = most common target value in the examples\n",
    "            attr = Counter(examples_vi['targets']).most_common(1)\n",
    "            label = 'Attribute: {!s}, {!s} occurences'.format(examples['attributes'][target_attribute], attr)\n",
    "            child.children.append(Node(label, []))\n",
    "        else:\n",
    "            # Else below this new branch add the subtree ID3 (Examples(vi), Attributes – {A})\n",
    "            child.children.append(\n",
    "                id3(examples_vi, \\\n",
    "                    [attribute_ for attribute_ in attributes if not attribute_ == attribute], \\\n",
    "                    attribute)\n",
    "            )\n",
    "    # End\n",
    "\n",
    "    # Return root\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the ID3 algorithm on the party data set which you already know from assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: late (gain 0.0933)\n",
      "    Value: yes\n",
      "        Result: no\n",
      "    Value: no\n",
      "        Attribute: distance (gain 0.5488)\n",
      "            Value: medium\n",
      "                Attribute: tired (gain 0.0)\n",
      "                    Value: yes\n",
      "                        Result: no\n",
      "                    Value: no\n",
      "                        Result: yes\n",
      "            Value: short\n",
      "                Result: yes\n",
      "            Value: long\n",
      "                Result: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('party.json', 'r') as party_file:\n",
    "    party = json.load(party_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "assert round(gain(party['targets'], [r[2] for r in party['data']]), 3) == 0.252\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_party = id3(party, list(range(len(party['attributes']))))\n",
    "\n",
    "print(tree_party)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the ID3 algorithm on the famous iris flowser data set, which you will hear more about in assignment 4 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: petal length (gain 0.5166)\n",
      "    Value: 1.4\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.5\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.6\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.7\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.0\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.1\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.2\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.3\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.9\n",
      "        Result: Iris-setosa\n",
      "    Value: 3.6\n",
      "        Result: Iris-versicolor\n",
      "    Value: 3.7\n",
      "        Result: Iris-versicolor\n",
      "    Value: 3.5\n",
      "        Result: Iris-versicolor\n",
      "    Value: 6.3\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.3\n",
      "        Result: Iris-versicolor\n",
      "    Value: 3.0\n",
      "        Result: Iris-versicolor\n",
      "    Value: 6.0\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.8\n",
      "        Result: Iris-versicolor\n",
      "    Value: 3.9\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.5\n",
      "        Attribute: sepal length (gain 0.5436)\n",
      "            Value: 5.4\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.6\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.7\n",
      "                Result: Iris-versicolor\n",
      "            Value: 4.9\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.4\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.2\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.0\n",
      "                Result: Iris-versicolor\n",
      "    Value: 4.4\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.7\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.6\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.1\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.0\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.3\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.2\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.8\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.9\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.1\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.9\n",
      "        Attribute: sepal width (gain 0.571)\n",
      "            Value: 2.8\n",
      "                Result: Iris-virginica\n",
      "            Value: 2.7\n",
      "                Result: Iris-virginica\n",
      "            Value: 3.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 2.5\n",
      "                Result: Iris-versicolor\n",
      "            Value: 3.1\n",
      "                Result: Iris-versicolor\n",
      "    Value: 4.8\n",
      "        Attribute: sepal length (gain 0.3113)\n",
      "            Value: 5.9\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.2\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.8\n",
      "                Result: Iris-versicolor\n",
      "    Value: 6.9\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.0\n",
      "        Attribute: sepal length (gain 0.8113)\n",
      "            Value: 6.7\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.3\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.7\n",
      "                Result: Iris-virginica\n",
      "    Value: 5.1\n",
      "        Attribute: sepal length (gain 0.1992)\n",
      "            Value: 5.8\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.9\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.5\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.3\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.0\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.9\n",
      "                Result: Iris-virginica\n",
      "    Value: 5.2\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.3\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.4\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.5\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.7\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.6\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.6\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.7\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.4\n",
      "        Result: Iris-virginica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('iris.json', 'r') as iris_file:\n",
    "    iris = json.load(iris_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "assert round(gain(iris['targets'], [r[2] for r in iris['data']]), 3) == 1.446\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_iris = id3(iris, list(range(len(iris['attributes']))))\n",
    "\n",
    "print(tree_iris)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The party-dataset is constructed from binary (nominal scale) attributes. It is therefore easy to build a (binary) decision-tree. With continuos attribute values like dimensions the range of the values has to be quantized first(e.g. x0 < 5 : x0 >= 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Decision Trees on Iris Flowers [5 Points]\n",
    "\n",
    "In this exercise we are going to examine and compare two decision trees that were generated from the iris flower data set ([Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set)) where three variations of the iris flower are quantified. The Iris data set is a classical example of a labeled dataset, i.e. every sample consists of two parts: features and labels. There are four features per sample in this data set (sepal length ($x_1$), sepal width ($x_2$), petal length ($x_2$) and petal width ($x_4$) in cm) and a corresponding label (Iris Setosa, Iris Versicolour, Iris Virginica). These samples are by nature **noisy**, no matter how carefully the measurement was taken - slight deviation from the actual length **cannot be avoided**. We want to learn how the features are related to the label so that we could (in the future) predict the label of a new sample automatically. One way to obtain such a `classifier` is to train a decision tree on the data.\n",
    "\n",
    "Here are two decisions tree generated by the data set. We will now take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree 1:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      +\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                     x3 < 4.95 |   x3 >= 4.95      +\n",
    "                        +--------------+       virginica\n",
    "                        |              |\n",
    "                        |              |\n",
    "              x4 < 1.65 | x4 >= 1.65   +\n",
    "                 +------------+    virginica\n",
    "                 |            |\n",
    "                 |            |\n",
    "                 +            +\n",
    "            versicolor    virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree 2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      +\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                               +                   +\n",
    "                          versicolor           virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "What does it mean that the features $x1$ and $x2$ do not appear in the decision trees?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data set shows that the sepal length and width of the different kinds of iris all lay in almost the same range. Therefore they do not contain any valuable information for distinguishing between the flowers; the entropy is close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "With which method from the lecture might the second tree have been generated from the first one? Explain the procedure."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "the sub-branch for x4 < 1.75 was pruned and the most common classification was asserted as leaf node: in this case \"versicolor\".\n",
    "Probably rule post pruning.\n",
    "\tFor rule post pruning, rules are made...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "After training the tree we can calculate the accuracy, i.e. the percentage of the training set that is classified correctly. Although the first tree was trained on the data set until no improvement of the accuracy was possible, its accuracy is *only* 98%. Explain why it is not 100 %"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The ranges of the attribute values of the different kinds of flowers overlap. Also, there is a lot of noise in the training data. Because of this, in some cases the algorithm is not able to correctly distinguish between the flowers and errors are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Tree 2 only has a 96% accuracy on the training set. Why might this tree still be preferable over tree 1?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tree 1 was made to nearly perfectly fit the training data, which most probably caused overfitting, meaning that the noise in the training data caused a too complex tree that has 98% accuracy over the training data but would have less accuracy when tested with new data. Tree 2 was pruned by cutting off a node. Now the accuracy for the training data is worse, but it will be more accurate when tested with new data because the “wrong” rules caused by overfitting are cut off.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
